{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpHT/SU63M2g+/lnZLgkVU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asnoldy02-cell/sds510/blob/main/Module_4_Essentials.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests pandas beautifulsoup4 mastodon.py lxml\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4cpEEQ8kkp8",
        "outputId": "aa989da6-01b3-4ea5-b626-e24dfa879207"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Collecting mastodon.py\n",
            "  Downloading mastodon_py-2.1.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (5.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Collecting python-magic (from mastodon.py)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: decorator>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mastodon.py) (4.4.2)\n",
            "Collecting blurhash>=1.1.4 (from mastodon.py)\n",
            "  Downloading blurhash-1.1.5-py2.py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading mastodon_py-2.1.4-py3-none-any.whl (123 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blurhash-1.1.5-py2.py3-none-any.whl (6.6 kB)\n",
            "Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Installing collected packages: blurhash, python-magic, mastodon.py\n",
            "Successfully installed blurhash-1.1.5 mastodon.py-2.1.4 python-magic-0.4.27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "from mastodon import Mastodon\n",
        "from bs4 import BeautifulSoup\n",
        "import random"
      ],
      "metadata": {
        "id": "SE8SLhtRkpDU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KEYWORDS = [\n",
        "    \"medicine\", \"medical\", \"drug\", \"therapy\", \"treatment\",\n",
        "    \"breakthrough\", \"research\", \"innovation\", \"health\", \"upcoming\"\n",
        "]\n",
        "OUTPUT_FILE = \"social_keyword_hits.csv\"\n",
        "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (compatible; OSINTMonitor/1.0)\"}\n"
      ],
      "metadata": {
        "id": "WDVZFdlnkqQg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_mastodon_posts(keywords):\n",
        "    print(\"üêò Fetching Mastodon posts...\")\n",
        "    results = []\n",
        "    try:\n",
        "        masto = Mastodon(api_base_url=\"https://mastodon.social\")\n",
        "        posts = masto.timeline_public(limit=80)\n",
        "        for post in posts:\n",
        "            content = BeautifulSoup(post[\"content\"], \"html.parser\").get_text()\n",
        "            for kw in keywords:\n",
        "                if kw.lower() in content.lower():\n",
        "                    results.append({\n",
        "                        \"source\": \"Mastodon\",\n",
        "                        \"keyword\": kw,\n",
        "                        \"content\": content[:250],\n",
        "                        \"url\": post[\"url\"],\n",
        "                        \"timestamp\": post[\"created_at\"]\n",
        "                    })\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Mastodon error: {e}\")\n",
        "    return results"
      ],
      "metadata": {
        "id": "uwNeiaFrks_3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_twitter_posts(keywords):\n",
        "    print(\"üê¶ Fetching Twitter (via Nitter)...\")\n",
        "    results = []\n",
        "    for kw in keywords:\n",
        "        url = f\"https://nitter.net/search?f=tweets&q={kw}\"\n",
        "        try:\n",
        "            resp = requests.get(url, headers=HEADERS, timeout=10)\n",
        "            if resp.status_code == 200:\n",
        "                soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "                tweets = soup.select(\"div.tweet-content\")\n",
        "                for t in tweets[:10]:\n",
        "                    text = t.get_text(\" \", strip=True)\n",
        "                    if kw.lower() in text.lower():\n",
        "                        results.append({\n",
        "                            \"source\": \"Twitter (Nitter)\",\n",
        "                            \"keyword\": kw,\n",
        "                            \"content\": text[:250],\n",
        "                            \"url\": url,\n",
        "                            \"timestamp\": datetime.now().isoformat()\n",
        "                        })\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Twitter/Nitter search error {resp.status_code} for '{kw}'\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Twitter error for '{kw}': {e}\")\n",
        "        time.sleep(2)\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "kDg1Kw6kkwDh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_telegram_posts(keywords):\n",
        "    print(\"üì± Fetching Telegram posts...\")\n",
        "    results = []\n",
        "    for kw in keywords:\n",
        "        search_url = f\"https://tgstat.com/en/search?query={kw}\"\n",
        "        try:\n",
        "            resp = requests.get(search_url, headers=HEADERS, timeout=10)\n",
        "            if resp.status_code == 200:\n",
        "                soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "                posts = soup.select(\"div.card-body\")\n",
        "                for p in posts[:10]:\n",
        "                    text = p.get_text(\" \", strip=True)\n",
        "                    link = p.find(\"a\", href=True)\n",
        "                    if kw.lower() in text.lower():\n",
        "                        results.append({\n",
        "                            \"source\": \"Telegram\",\n",
        "                            \"keyword\": kw,\n",
        "                            \"content\": text[:250],\n",
        "                            \"url\": link[\"href\"] if link else search_url,\n",
        "                            \"timestamp\": datetime.now().isoformat()\n",
        "                        })\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Telegram search error {resp.status_code} for '{kw}'\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Telegram error for '{kw}': {e}\")\n",
        "        time.sleep(2)\n",
        "    return results"
      ],
      "metadata": {
        "id": "0xeDAXrok2uo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_parler_posts(keywords):\n",
        "    print(\"ü¶ú Fetching Parler posts (DuckDuckGo)...\")\n",
        "    results = []\n",
        "    for kw in keywords:\n",
        "        search_url = f\"https://duckduckgo.com/html/?q=site:parler.com {kw}\"\n",
        "        try:\n",
        "            resp = requests.get(search_url, headers=HEADERS, timeout=10)\n",
        "            soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "            links = soup.select(\"a.result__a\")\n",
        "            for l in links[:5]:\n",
        "                text = l.get_text(\" \", strip=True)\n",
        "                if kw.lower() in text.lower():\n",
        "                    results.append({\n",
        "                        \"source\": \"Parler\",\n",
        "                        \"keyword\": kw,\n",
        "                        \"content\": text[:250],\n",
        "                        \"url\": l[\"href\"],\n",
        "                        \"timestamp\": datetime.now().isoformat()\n",
        "                    })\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Parler error: {e}\")\n",
        "        time.sleep(2)\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "eWRG5PW3k5l0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_truthsocial_posts(keywords):\n",
        "    print(\"üïä Fetching Truth Social posts (DuckDuckGo)...\")\n",
        "    results = []\n",
        "    for kw in keywords:\n",
        "        search_url = f\"https://duckduckgo.com/html/?q=site:truthsocial.com {kw}\"\n",
        "        try:\n",
        "            resp = requests.get(search_url, headers=HEADERS, timeout=10)\n",
        "            soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "            links = soup.select(\"a.result__a\")\n",
        "            for l in links[:5]:\n",
        "                text = l.get_text(\" \", strip=True)\n",
        "                if kw.lower() in text.lower():\n",
        "                    results.append({\n",
        "                        \"source\": \"Truth Social\",\n",
        "                        \"keyword\": kw,\n",
        "                        \"content\": text[:250],\n",
        "                        \"url\": l[\"href\"],\n",
        "                        \"timestamp\": datetime.now().isoformat()\n",
        "                    })\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Truth Social error: {e}\")\n",
        "        time.sleep(2)\n",
        "    return results"
      ],
      "metadata": {
        "id": "sEnLJ72Zk6Y3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_mewe_posts(keywords):\n",
        "    print(\"üåê Fetching MeWe posts (DuckDuckGo)...\")\n",
        "    results = []\n",
        "    for kw in keywords:\n",
        "        search_url = f\"https://duckduckgo.com/html/?q=site:mewe.com {kw}\"\n",
        "        try:\n",
        "            resp = requests.get(search_url, headers=HEADERS, timeout=10)\n",
        "            soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "            links = soup.select(\"a.result__a\")\n",
        "            for l in links[:5]:\n",
        "                text = l.get_text(\" \", strip=True)\n",
        "                if kw.lower() in text.lower():\n",
        "                    results.append({\n",
        "                        \"source\": \"MeWe\",\n",
        "                        \"keyword\": kw,\n",
        "                        \"content\": text[:250],\n",
        "                        \"url\": l[\"href\"],\n",
        "                        \"timestamp\": datetime.now().isoformat()\n",
        "                    })\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è MeWe error: {e}\")\n",
        "        time.sleep(2)\n",
        "    return results"
      ],
      "metadata": {
        "id": "nlclfAiYk99T"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_results = []\n",
        "for func in [\n",
        "    fetch_mastodon_posts,\n",
        "    fetch_twitter_posts,\n",
        "    fetch_telegram_posts,\n",
        "    fetch_parler_posts,\n",
        "    fetch_truthsocial_posts,\n",
        "    fetch_mewe_posts\n",
        "]:\n",
        "    all_results += func(KEYWORDS)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ykdV6pKlAJU",
        "outputId": "9c66b106-6dd0-418d-c3a6-4a6ea54358a2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üêò Fetching Mastodon posts...\n",
            "üê¶ Fetching Twitter (via Nitter)...\n",
            "‚ö†Ô∏è Twitter error for 'medicine': ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "‚ö†Ô∏è Twitter error for 'medical': ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "‚ö†Ô∏è Twitter error for 'drug': ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "‚ö†Ô∏è Twitter error for 'therapy': ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "‚ö†Ô∏è Twitter error for 'treatment': ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "‚ö†Ô∏è Twitter error for 'breakthrough': ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "‚ö†Ô∏è Twitter error for 'research': ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "‚ö†Ô∏è Twitter error for 'innovation': ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "‚ö†Ô∏è Twitter error for 'health': ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "‚ö†Ô∏è Twitter error for 'upcoming': ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "üì± Fetching Telegram posts...\n",
            "ü¶ú Fetching Parler posts (DuckDuckGo)...\n",
            "üïä Fetching Truth Social posts (DuckDuckGo)...\n",
            "üåê Fetching MeWe posts (DuckDuckGo)...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if all_results:\n",
        "    df = pd.DataFrame(all_results)\n",
        "    if os.path.exists(OUTPUT_FILE):\n",
        "        df.to_csv(OUTPUT_FILE, mode=\"a\", header=False, index=False, encoding=\"utf-8\")\n",
        "    else:\n",
        "        df.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8\")\n",
        "    print(f\"‚úÖ {len(df)} results found and saved to {OUTPUT_FILE}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No keyword hits found this time.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s62KkDkzli9r",
        "outputId": "49cff709-427e-40ce-c511-f75a57beb14a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ 3 results found and saved to social_keyword_hits.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"social_keyword_hits.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "8iGqevEAlnR3",
        "outputId": "ec1d3a58-ddd6-4785-8395-f787a2e27fa5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b07ed74e-9836-4044-b886-65de730960b5\", \"social_keyword_hits.csv\", 1045)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}